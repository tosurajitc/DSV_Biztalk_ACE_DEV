# ================================================================================================
# GENERIC PDF_PROCESSOR.PY - 100% ADAPTIVE FOR 1000+ BUSINESS FLOWS
# ================================================================================================
# 
# DESIGN PRINCIPLES:
# 1. ZERO hardcoded patterns - all detection is adaptive and LLM-driven
# 2. 100% generic - works for ANY business domain (BizTalk, SAP, Oracle, etc.)
# 3. Self-learning - adapts to document structure automatically
# 4. Pattern-agnostic - discovers patterns from content, not predefined lists
# 5. Scale-ready - optimized for 1000+ different flow types
#
# ================================================================================================

import PyPDF2
import json
import re
from datetime import datetime
from typing import List, Dict, Tuple, Optional, Any
import easyocr
import fitz  # PyMuPDF
import cv2
import numpy as np
from PIL import Image
import base64
import io
import tempfile
import os
import streamlit as st
import pdfplumber
import pandas as pd
from dataclasses import dataclass, asdict

# ================================================================================================
# ADAPTIVE CONFIGURATION SYSTEM
# ================================================================================================

@dataclass
class ProcessingConfig:
    """Adaptive configuration that learns from document structure"""
    min_section_length: int = 100
    max_section_length: int = 5000
    similarity_threshold: float = 0.7
    confidence_threshold: float = 0.6
    table_detection_sensitivity: float = 0.8
    diagram_detection_threshold: float = 0.7
    
    def adapt_to_document(self, doc_stats: Dict):
        """Automatically adapt configuration based on document characteristics"""
        avg_section_length = doc_stats.get('avg_section_length', 1000)
        
        # Adaptive thresholds based on document complexity
        if avg_section_length > 3000:  # Long technical documents
            self.max_section_length = min(8000, avg_section_length * 2)
            self.similarity_threshold = 0.8
        elif avg_section_length < 500:  # Concise documents
            self.min_section_length = 50
            self.similarity_threshold = 0.6

@dataclass
class BusinessBlock:
    """Generic business block structure"""
    block_type: str
    content: str
    extracted_data: Dict
    confidence: float
    metadata: Dict
    section_context: str

# ================================================================================================
# MAIN PDF PROCESSOR CLASS
# ================================================================================================

class AdaptivePDFProcessor:
    """
    100% Generic PDF processor that adapts to ANY business domain
    No hardcoded patterns - discovers structure from content
    """
    
    def __init__(self):
        """Initialize with adaptive capabilities"""
        self.config = ProcessingConfig()
        self.groq_client = self._initialize_groq_client()
        self.groq_model = "llama-3.1-70b-versatile"
        self.vision_model = "llama-3.2-90b-vision-preview"
        
        # Adaptive learning components
        self.pattern_cache = {}
        self.structure_memory = {}
        self.confidence_tracker = {}
        
        # Processing state
        self.current_pdf_path = None
        self.document_structure = None
        self.processing_stats = {
            'total_sections': 0,
            'business_blocks_found': 0,
            'diagrams_processed': 0,
            'tables_extracted': 0,
            'patterns_discovered': 0
        }
        
        # Initialize OCR
        try:
            self.ocr_reader = easyocr.Reader(['en'])
            print("âœ… EasyOCR initialized successfully")
        except Exception as e:
            print(f"âš ï¸ EasyOCR initialization failed: {e}")
            self.ocr_reader = None

    def _initialize_groq_client(self):
        """Initialize Groq client for LLM processing"""
        try:
            from groq import Groq
            api_key = os.getenv('GROQ_API_KEY')
            if not api_key:
                return None
            return Groq(api_key=api_key)
        except Exception:
            return None



    def _extract_methods_from_content(self, content: str) -> List[Dict]:
        """Extract SOAP/WS methods from business requirements"""
        methods = []
        
        # Pattern 1: Method names ending with WS/Service
        method_patterns = [
            r'\b([a-zA-Z]+(?:WS|Service|Method|Operation))\b',
            r'(\w+\.(?:Snd|Rec)\.WCF)',
            r'(subscription|confirm|cancel|submit)\w*'
        ]
        
        for pattern in method_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                methods.append({
                    'name': match,
                    'type': 'soap_operation',
                    'source': 'pdf_extraction'
                })
        
        # Pattern 2: Look for "5 methods" or "multiple operations"
        if re.search(r'(\d+)\s+methods?\s+should\s+be\s+trigger', content):
            print(f"âœ“ Multi-method pattern detected")
        
        return list({m['name']: m for m in methods}.values())  # Deduplicate


    # ================================================================================================
    # MAIN PROCESSING PIPELINE - 100% ADAPTIVE
    # ================================================================================================

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        MAIN PIPELINE: Adaptive extraction for ANY business document type
        """
        self.current_pdf_path = pdf_path
        
        try:
            print(f"ðŸš€ Starting adaptive processing for: {os.path.basename(pdf_path)}")
            
            # Step 1: Multi-modal content extraction
            raw_content = self._extract_multi_modal_content(pdf_path)
            
            # Step 2: Discover document structure automatically
            self.document_structure = self._discover_document_structure(raw_content)
            
            # Step 3: Adapt configuration to this document type
            self.config.adapt_to_document(self.document_structure['stats'])
            
            # Step 4: Create adaptive chunks based on discovered structure
            adaptive_chunks = self._create_adaptive_chunks(raw_content)
            
            # Step 5: Discover and extract business patterns automatically
            business_blocks = self._discover_business_patterns(adaptive_chunks)
            
            # Step 6: Enhance chunks with discovered business data
            enhanced_chunks = self._enhance_chunks_with_business_data(adaptive_chunks, business_blocks)
            
            # Step 7: Prepare for vector store with adaptive metadata
            vector_ready_chunks = self._prepare_adaptive_vector_chunks(enhanced_chunks)
            
            # Store results and update statistics
            self._update_processing_statistics(vector_ready_chunks, business_blocks)
            
            print(f"âœ… Adaptive processing complete: {len(vector_ready_chunks)} chunks, {len(business_blocks)} business blocks")
            
            return raw_content['text']
            
        except Exception as e:
            print(f"âŒ Adaptive processing failed: {str(e)}")
            raise PDFProcessingError(f"Adaptive PDF processing failed: {str(e)}")

    # ================================================================================================
    # STEP 1: MULTI-MODAL CONTENT EXTRACTION
    # ================================================================================================

    def _extract_multi_modal_content(self, pdf_path: str) -> Dict:
        """Extract text, tables, and diagrams using multiple methods"""
        content = {
            'text': '',
            'tables': [],
            'diagrams': [],
            'metadata': {}
        }
        
        # Method 1: Advanced text extraction with pdfplumber
        content.update(self._extract_text_with_pdfplumber(pdf_path))
        
        # Method 2: Diagram extraction with vision analysis
        diagrams = self._extract_diagrams_with_vision(pdf_path)
        content['diagrams'].extend(diagrams)
        
        # Method 3: Structured table extraction
        tables = self._extract_structured_tables(pdf_path)
        content['tables'].extend(tables)
        
        return content

    def _extract_text_with_pdfplumber(self, pdf_path: str) -> Dict:
        """Advanced text extraction preserving structure"""
        content = {'text': '', 'layout_info': [], 'metadata': {}}
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    # Extract text with layout information
                    page_text = page.extract_text()
                    if page_text:
                        content['text'] += f"\n\n[PAGE {page_num + 1}]\n{page_text}"
                    
                    # Extract layout elements
                    layout_elements = {
                        'page': page_num + 1,
                        'chars': len(page.chars) if hasattr(page, 'chars') else 0,
                        'words': len(page_text.split()) if page_text else 0,
                        'lines': len(page_text.split('\n')) if page_text else 0
                    }
                    content['layout_info'].append(layout_elements)
                    
                content['metadata']['total_pages'] = len(pdf.pages)
                content['metadata']['total_words'] = sum(info['words'] for info in content['layout_info'])
                
        except Exception as e:
            print(f"âš ï¸ pdfplumber extraction failed: {e}")
            # Fallback to PyPDF2
            content = self._extract_text_fallback(pdf_path)
            
        return content

    def _extract_text_fallback(self, pdf_path: str) -> Dict:
        """Fallback text extraction using PyPDF2"""
        content = {'text': '', 'layout_info': [], 'metadata': {}}
        
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num, page in enumerate(pdf_reader.pages):
                    page_text = page.extract_text()
                    if page_text:
                        content['text'] += f"\n\n[PAGE {page_num + 1}]\n{page_text}"
                        
                content['metadata']['total_pages'] = len(pdf_reader.pages)
                content['metadata']['extraction_method'] = 'PyPDF2_fallback'
                
        except Exception as e:
            raise PDFProcessingError(f"All text extraction methods failed: {str(e)}")
            
        return content

    def _extract_structured_tables(self, pdf_path: str) -> List[Dict]:
        """Extract tables with adaptive detection"""
        tables = []
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    # Adaptive table detection
                    page_tables = page.extract_tables()
                    
                    for table_idx, table in enumerate(page_tables):
                        if table and len(table) > 1:  # Must have header + data
                            table_data = {
                                'page': page_num + 1,
                                'table_index': table_idx,
                                'data': table,
                                'structure': self._analyze_table_structure(table),
                                'business_context': self._extract_table_context(page, table)
                            }
                            tables.append(table_data)
                            
        except Exception as e:
            print(f"âš ï¸ Table extraction failed: {e}")
            
        return tables

    def _analyze_table_structure(self, table: List[List]) -> Dict:
        """Analyze table structure to understand its purpose"""
        if not table or len(table) < 2:
            return {'type': 'invalid', 'confidence': 0.0}
        
        headers = table[0] if table[0] else []
        data_rows = table[1:] if len(table) > 1 else []
        
        structure = {
            'rows': len(table),
            'columns': len(headers),
            'headers': headers,
            'data_density': self._calculate_data_density(data_rows),
            'column_types': self._infer_column_types(data_rows),
            'potential_business_type': self._classify_table_type(headers)
        }
        
        return structure

    def _calculate_data_density(self, data_rows: List[List]) -> float:
        """Calculate how much of the table contains actual data"""
        if not data_rows:
            return 0.0
        
        total_cells = sum(len(row) for row in data_rows)
        filled_cells = sum(1 for row in data_rows for cell in row if cell and str(cell).strip())
        
        return filled_cells / total_cells if total_cells > 0 else 0.0

    def _infer_column_types(self, data_rows: List[List]) -> List[str]:
        """Infer data types for each column"""
        if not data_rows:
            return []
        
        max_cols = max(len(row) for row in data_rows) if data_rows else 0
        column_types = []
        
        for col_idx in range(max_cols):
            col_values = [row[col_idx] if col_idx < len(row) else '' for row in data_rows]
            col_values = [str(val).strip() for val in col_values if val]
            
            if not col_values:
                column_types.append('empty')
                continue
            
            # Type inference logic
            if all(self._is_numeric(val) for val in col_values[:5]):  # Check first 5 values
                column_types.append('numeric')
            elif all(self._is_date(val) for val in col_values[:5]):
                column_types.append('date')
            elif any(keyword in ''.join(col_values).lower() for keyword in ['queue', 'topic', 'channel']):
                column_types.append('messaging')
            elif any(keyword in ''.join(col_values).lower() for keyword in ['database', 'table', 'procedure']):
                column_types.append('database')
            else:
                column_types.append('text')
                
        return column_types

    def _classify_table_type(self, headers: List[str]) -> str:
        """Classify table type based on headers"""
        if not headers:
            return 'unknown'
        
        headers_text = ' '.join(str(h).lower() for h in headers if h)
        
        # Adaptive classification based on header content
        if any(term in headers_text for term in ['mq', 'queue', 'topic', 'channel']):
            return 'messaging_configuration'
        elif any(term in headers_text for term in ['event', 'tracking', 'log']):
            return 'event_data'
        elif any(term in headers_text for term in ['route', 'routing', 'destination']):
            return 'routing_table'
        elif any(term in headers_text for term in ['performance', 'sla', 'throughput', 'latency']):
            return 'performance_requirements'
        elif any(term in headers_text for term in ['migration', 'component', 'mapping']):
            return 'migration_overview'
        elif any(term in headers_text for term in ['test', 'scenario', 'case', 'result']):
            return 'test_data'
        else:
            return 'business_data'

    def _extract_table_context(self, page, table: List[List]) -> Dict:
        """Extract context around table for better understanding"""
        # This is a simplified version - in practice, you'd analyze surrounding text
        context = {
            'preceding_text': '',  # Text before table
            'following_text': '',  # Text after table
            'section_header': '',  # Section this table belongs to
            'page_context': ''     # Overall page context
        }
        
        # Extract text around table location (simplified)
        page_text = page.extract_text() or ''
        context['page_context'] = page_text[:500]  # First 500 chars as context
        
        return context

    # ================================================================================================
    # STEP 2: DOCUMENT STRUCTURE DISCOVERY
    # ================================================================================================

    def _discover_document_structure(self, content: Dict) -> Dict:
        """Automatically discover document structure without hardcoded patterns"""
        text = content.get('text', '')
        
        structure = {
            'sections': self._discover_sections(text),
            'patterns': self._discover_content_patterns(text),
            'stats': self._calculate_document_stats(content),
            'business_indicators': self._discover_business_indicators(text),
            'document_type': self._classify_document_type(content)
        }
        
        return structure

    def _discover_sections(self, text: str) -> List[Dict]:
        """Discover document sections automatically using LLM analysis"""
        if not self.groq_client:
            return self._discover_sections_heuristic(text)
        
        try:
            # Use LLM to discover section structure
            prompt = f"""Analyze this document and identify its natural section structure. Return a JSON list of sections with their start indicators.

Document text (first 3000 characters):
{text[:3000]}

Requirements:
1. Identify natural section breaks (headers, topics changes, formatting changes)
2. Don't assume any specific format - discover the actual structure
3. Return section indicators that appear in the text
4. Include confidence scores

Return JSON format:
[
  {{
    "section_name": "discovered_name",
    "start_indicator": "actual_text_that_starts_section",
    "confidence": 0.95,
    "section_type": "header|content|table|list"
  }}
]
"""
            
            response = self.groq_client.chat.completions.create(
                model=self.groq_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2000,
                temperature=0.1
            )
            
            # Track token usage
            if 'token_tracker' in st.session_state and hasattr(response, 'usage') and response.usage:
                st.session_state.token_tracker.manual_track(
                    agent="pdf_processor",
                    operation="section_discovery",
                    model=self.groq_model,
                    input_tokens=response.usage.prompt_tokens,
                    output_tokens=response.usage.completion_tokens,
                    flow_name="adaptive_pdf_processing"
                )
            
            # Parse response
            result_text = response.choices[0].message.content
            try:
                sections = json.loads(result_text)
                return sections if isinstance(sections, list) else []
            except json.JSONDecodeError:
                print("âš ï¸ LLM section discovery returned invalid JSON, using heuristic")
                return self._discover_sections_heuristic(text)
                
        except Exception as e:
            print(f"âš ï¸ LLM section discovery failed: {e}")
            return self._discover_sections_heuristic(text)

    def _discover_sections_heuristic(self, text: str) -> List[Dict]:
        """Fallback heuristic section discovery"""
        sections = []
        lines = text.split('\n')
        
        current_section = None
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # Detect potential section headers (various patterns)
            if self._is_potential_header(line_stripped, i, lines):
                if current_section:
                    sections.append(current_section)
                
                current_section = {
                    'section_name': line_stripped[:50],  # First 50 chars as name
                    'start_indicator': line_stripped,
                    'confidence': self._calculate_header_confidence(line_stripped),
                    'section_type': 'header',
                    'line_number': i
                }
        
        # Add final section
        if current_section:
            sections.append(current_section)
        
        return sections

    def _is_potential_header(self, line: str, line_idx: int, all_lines: List[str]) -> bool:
        """Detect if a line is likely a section header"""
        if not line or len(line) < 3:
            return False
        
        # Various header indicators
        header_indicators = [
            line.isupper() and len(line) < 100,  # ALL CAPS, reasonable length
            line.endswith(':') and len(line) < 80,  # Ends with colon
            bool(re.match(r'^\d+\.?\s+[A-Z]', line)),  # Starts with number
            bool(re.match(r'^[A-Z][a-z]+ [A-Z]', line)),  # Title Case
            line.count(' ') < 6 and line[0].isupper(),  # Short, starts with capital
        ]
        
        # Context indicators
        if line_idx > 0 and line_idx < len(all_lines) - 1:
            prev_line = all_lines[line_idx - 1].strip()
            next_line = all_lines[line_idx + 1].strip()
            
            # Surrounded by empty lines
            if not prev_line and not next_line:
                header_indicators.append(True)
            
            # Different formatting from surrounding lines
            if prev_line and next_line:
                avg_surrounding_length = (len(prev_line) + len(next_line)) / 2
                if len(line) < avg_surrounding_length * 0.6:  # Much shorter
                    header_indicators.append(True)
        
        return sum(header_indicators) >= 2  # Require at least 2 indicators

    def _calculate_header_confidence(self, line: str) -> float:
        """Calculate confidence that a line is a header"""
        confidence = 0.5  # Base confidence
        
        # Length factor
        if 10 <= len(line) <= 60:
            confidence += 0.2
        
        # Capitalization
        if line[0].isupper():
            confidence += 0.1
        
        # Ends with colon
        if line.endswith(':'):
            confidence += 0.2
        
        # Contains numbers (section numbering)
        if re.search(r'\d+', line):
            confidence += 0.1
        
        return min(confidence, 0.95)

    def _discover_content_patterns(self, text: str) -> Dict:
        """Discover recurring patterns in content automatically"""
        patterns = {
            'list_patterns': self._find_list_patterns(text),
            'table_patterns': self._find_table_patterns(text),
            'reference_patterns': self._find_reference_patterns(text),
            'technical_patterns': self._find_technical_patterns(text),
            'business_patterns': self._find_business_patterns(text)
        }
        
        return patterns

    def _find_list_patterns(self, text: str) -> List[Dict]:
        """Find list-like patterns in text"""
        patterns = []
        
        # Bullet point patterns
        bullet_pattern = re.compile(r'^[\s]*[-â€¢*â—¦â–ªâ–«]\s+(.+)$', re.MULTILINE)
        bullet_matches = bullet_pattern.findall(text)
        if len(bullet_matches) >= 3:
            patterns.append({
                'type': 'bullet_list',
                'count': len(bullet_matches),
                'sample': bullet_matches[:3],
                'confidence': min(0.9, len(bullet_matches) / 10)
            })
        
        # Numbered list patterns
        numbered_pattern = re.compile(r'^[\s]*\d+\.?\s+(.+)$', re.MULTILINE)
        numbered_matches = numbered_pattern.findall(text)
        if len(numbered_matches) >= 3:
            patterns.append({
                'type': 'numbered_list',
                'count': len(numbered_matches),
                'sample': numbered_matches[:3],
                'confidence': min(0.9, len(numbered_matches) / 10)
            })
        
        return patterns

    def _find_table_patterns(self, text: str) -> List[Dict]:
        """Find table-like patterns in text"""
        patterns = []
        
        # Look for pipe-separated content
        pipe_pattern = re.compile(r'^[^|\n]*\|[^|\n]*\|[^|\n]*', re.MULTILINE)
        pipe_matches = pipe_pattern.findall(text)
        if len(pipe_matches) >= 2:
            patterns.append({
                'type': 'pipe_table',
                'count': len(pipe_matches),
                'sample': pipe_matches[:2],
                'confidence': 0.8
            })
        
        # Look for tab-separated content
        tab_pattern = re.compile(r'^[^\t\n]*\t[^\t\n]*\t[^\t\n]*', re.MULTILINE)
        tab_matches = tab_pattern.findall(text)
        if len(tab_matches) >= 2:
            patterns.append({
                'type': 'tab_table',
                'count': len(tab_matches),
                'sample': tab_matches[:2],
                'confidence': 0.7
            })
        
        return patterns

    def _find_reference_patterns(self, text: str) -> List[Dict]:
        """Find reference patterns (IDs, codes, names)"""
        patterns = []
        
        # File extensions
        file_pattern = re.compile(r'\b\w+\.\w{2,4}\b')
        file_matches = list(set(file_pattern.findall(text)))
        if len(file_matches) >= 3:
            patterns.append({
                'type': 'file_references',
                'count': len(file_matches),
                'sample': file_matches[:5],
                'confidence': 0.9
            })
        
        # URLs
        url_pattern = re.compile(r'https?://[^\s]+')
        url_matches = url_pattern.findall(text)
        if url_matches:
            patterns.append({
                'type': 'url_references',
                'count': len(url_matches),
                'sample': url_matches[:3],
                'confidence': 0.95
            })
        
        # Code-like patterns (alphanumeric with dots/underscores)
        code_pattern = re.compile(r'\b[A-Z][A-Z0-9_]+\.[A-Z][A-Z0-9_]+\b')
        code_matches = list(set(code_pattern.findall(text)))
        if len(code_matches) >= 3:
            patterns.append({
                'type': 'code_references',
                'count': len(code_matches),
                'sample': code_matches[:5],
                'confidence': 0.8
            })
        
        return patterns

    def _find_technical_patterns(self, text: str) -> List[Dict]:
        """Find technical patterns automatically"""
        patterns = []
        
        # Database-related patterns
        db_keywords = ['database', 'table', 'query', 'select', 'insert', 'update', 'procedure', 'schema']
        db_count = sum(text.lower().count(keyword) for keyword in db_keywords)
        if db_count >= 5:
            patterns.append({
                'type': 'database_context',
                'count': db_count,
                'confidence': min(0.9, db_count / 20)
            })
        
        # Messaging patterns
        msg_keywords = ['queue', 'topic', 'message', 'publish', 'subscribe', 'broker', 'channel']
        msg_count = sum(text.lower().count(keyword) for keyword in msg_keywords)
        if msg_count >= 5:
            patterns.append({
                'type': 'messaging_context',
                'count': msg_count,
                'confidence': min(0.9, msg_count / 20)
            })
        
        # Integration patterns
        int_keywords = ['integration', 'adapter', 'connector', 'endpoint', 'service', 'api', 'transform']
        int_count = sum(text.lower().count(keyword) for keyword in int_keywords)
        if int_count >= 5:
            patterns.append({
                'type': 'integration_context',
                'count': int_count,
                'confidence': min(0.9, int_count / 20)
            })
        
        return patterns

    def _find_business_patterns(self, text: str) -> List[Dict]:
        """Find business-specific patterns automatically"""
        patterns = []
        
        # Performance/SLA patterns
        perf_keywords = ['performance', 'sla', 'throughput', 'latency', 'response time', 'availability']
        perf_count = sum(text.lower().count(keyword) for keyword in perf_keywords)
        if perf_count >= 3:
            patterns.append({
                'type': 'performance_requirements',
                'count': perf_count,
                'confidence': min(0.9, perf_count / 15)
            })
        
        # Migration patterns
        mig_keywords = ['migration', 'migrate', 'legacy', 'modernize', 'upgrade', 'transition']
        mig_count = sum(text.lower().count(keyword) for keyword in mig_keywords)
        if mig_count >= 3:
            patterns.append({
                'type': 'migration_context',
                'count': mig_count,
                'confidence': min(0.9, mig_count / 15)
            })
        
        return patterns

    def _calculate_document_stats(self, content: Dict) -> Dict:
        """Calculate adaptive document statistics"""
        text = content.get('text', '')
        
        lines = text.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]
        
        stats = {
            'total_chars': len(text),
            'total_words': len(text.split()),
            'total_lines': len(lines),
            'non_empty_lines': len(non_empty_lines),
            'avg_line_length': sum(len(line) for line in non_empty_lines) / len(non_empty_lines) if non_empty_lines else 0,
            'avg_section_length': 0,  # Will be calculated after section discovery
            'table_count': len(content.get('tables', [])),
            'diagram_count': len(content.get('diagrams', [])),
            'complexity_score': self._calculate_complexity_score(text)
        }
        
        return stats

    def _calculate_complexity_score(self, text: str) -> float:
        """Calculate document complexity for adaptive processing"""
        factors = []
        
        # Length factor
        length_score = min(1.0, len(text) / 50000)  # Normalize to 50k chars
        factors.append(length_score)
        
        # Technical term density
        technical_terms = ['database', 'integration', 'transformation', 'migration', 'configuration']
        term_count = sum(text.lower().count(term) for term in technical_terms)
        term_density = min(1.0, term_count / 100)
        factors.append(term_density)
        
        # Structure complexity (varied line lengths, formatting)
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        if lines:
            length_variance = np.var([len(line) for line in lines]) if len(lines) > 1 else 0
            structure_score = min(1.0, length_variance / 10000)
            factors.append(structure_score)
        
        return sum(factors) / len(factors)

    def _discover_business_indicators(self, text: str) -> List[str]:
        """Discover business indicators automatically without hardcoded lists"""
        if not self.groq_client:
            return self._discover_business_indicators_heuristic(text)
        
        try:
            # Use LLM to discover business indicators
            prompt = f"""Analyze this business document and identify the key business/technical indicators present. Focus on discovering actual patterns rather than assuming predefined categories.

Document excerpt (first 2000 characters):
{text[:2000]}

Instructions:
1. Identify business process terms that appear in the text
2. Find technical implementation concepts mentioned
3. Discover domain-specific terminology
4. Extract process flow indicators
5. Identify data/system integration terms

Return a JSON list of discovered indicators with confidence scores:
[
  {{
    "indicator": "discovered_term",
    "category": "business_process|technical_implementation|data_integration|system_architecture",
    "frequency": number_of_occurrences,
    "confidence": 0.95
  }}
]
"""
            
            response = self.groq_client.chat.completions.create(
                model=self.groq_model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.1
            )
            
            # Track token usage
            if 'token_tracker' in st.session_state and hasattr(response, 'usage') and response.usage:
                st.session_state.token_tracker.manual_track(
                    agent="pdf_processor",
                    operation="business_indicator_discovery",
                    model=self.groq_model,
                    input_tokens=response.usage.prompt_tokens,
                    output_tokens=response.usage.completion_tokens,
                    flow_name="adaptive_pdf_processing"
                )
            
            # Parse response
            result_text = response.choices[0].message.content
            try:
                indicators_data = json.loads(result_text)
                if isinstance(indicators_data, list):
                    return [item['indicator'] for item in indicators_data if item.get('confidence', 0) > 0.6]
                else:
                    return []
            except json.JSONDecodeError:
                print("âš ï¸ LLM business indicator discovery returned invalid JSON")
                return self._discover_business_indicators_heuristic(text)
                
        except Exception as e:
            print(f"âš ï¸ LLM business indicator discovery failed: {e}")
            return self._discover_business_indicators_heuristic(text)

    def _discover_business_indicators_heuristic(self, text: str) -> List[str]:
        """Fallback heuristic business indicator discovery"""
        indicators = []
        
        # Extract frequent meaningful terms
        words = re.findall(r'\b[A-Za-z]{4,}\b', text.lower())  # Words 4+ chars
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Filter for potential business terms (frequency > 3, not common words)
        common_words = {'that', 'this', 'with', 'from', 'they', 'were', 'been', 'have', 'will', 'would', 'could', 'should'}
        
        for word, freq in word_freq.items():
            if freq >= 3 and word not in common_words and len(word) >= 4:
                indicators.append(word)
        
        # Sort by frequency and take top indicators
        indicators.sort(key=lambda w: word_freq[w], reverse=True)
        return indicators[:20]  # Top 20 discovered indicators

    def _classify_document_type(self, content: Dict) -> str:
        """Classify document type automatically"""
        text = content.get('text', '').lower()
        
        # Analyze content characteristics
        technical_score = 0
        business_score = 0
        migration_score = 0
        
        # Technical indicators
        if any(term in text for term in ['database', 'queue', 'integration', 'api', 'service']):
            technical_score += 1
        
        # Business indicators
        if any(term in text for term in ['requirements', 'process', 'workflow', 'business']):
            business_score += 1
        
        # Migration indicators
        if any(term in text for term in ['migration', 'legacy', 'modernize', 'transition']):
            migration_score += 1
        
        # Determine primary type
        scores = {
            'technical_specification': technical_score,
            'business_requirements': business_score,
            'migration_document': migration_score
        }
        
        return max(scores.items(), key=lambda x: x[1])[0] if max(scores.values()) > 0 else 'general_document'

    # ================================================================================================
    # STEP 3: ADAPTIVE CHUNKING
    # ================================================================================================

    def _create_adaptive_chunks(self, content: Dict) -> List[Dict]:
        """Create chunks that adapt to document structure"""
        text = content.get('text', '')
        
        if not text:
            return []
        
        # Use discovered structure for intelligent chunking
        if self.document_structure and self.document_structure.get('sections'):
            chunks = self._chunk_by_discovered_sections(text, self.document_structure['sections'])
        else:
            chunks = self._chunk_by_adaptive_strategy(text)
        
        # Enhance chunks with metadata
        enhanced_chunks = []
        for i, chunk in enumerate(chunks):
            enhanced_chunk = {
                'chunk_id': i,
                'content': chunk['content'],
                'section': chunk.get('section', f'section_{i}'),
                'metadata': self._generate_adaptive_metadata(chunk, content),
                'business_context': self._extract_business_context(chunk['content'])
            }
            enhanced_chunks.append(enhanced_chunk)
        
        return enhanced_chunks

    def _chunk_by_discovered_sections(self, text: str, sections: List[Dict]) -> List[Dict]:
        """Chunk text based on discovered sections"""
        chunks = []
        text_lines = text.split('\n')
        
        for i, section in enumerate(sections):
            start_indicator = section.get('start_indicator', '')
            
            # Find section start
            start_line = 0
            for line_idx, line in enumerate(text_lines):
                if start_indicator.lower() in line.lower():
                    start_line = line_idx
                    break
            
            # Find section end (start of next section or end of document)
            end_line = len(text_lines)
            if i + 1 < len(sections):
                next_indicator = sections[i + 1].get('start_indicator', '')
                for line_idx in range(start_line + 1, len(text_lines)):
                    if next_indicator.lower() in text_lines[line_idx].lower():
                        end_line = line_idx
                        break
            
            # Extract section content
            section_lines = text_lines[start_line:end_line]
            section_content = '\n'.join(section_lines).strip()
            
            if section_content and len(section_content) > 50:  # Minimum content threshold
                chunk = {
                    'content': section_content,
                    'section': section.get('section_name', f'section_{i}'),
                    'section_type': section.get('section_type', 'content'),
                    'confidence': section.get('confidence', 0.5)
                }
                chunks.append(chunk)
        
        return chunks

    def _chunk_by_adaptive_strategy(self, text: str) -> List[Dict]:
        """Adaptive chunking when sections are not clearly defined"""
        chunks = []
        
        # Split by paragraphs first
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        current_chunk = []
        current_length = 0
        chunk_id = 0
        
        for paragraph in paragraphs:
            paragraph_length = len(paragraph)
            
            # Adaptive size based on content complexity
            max_chunk_size = self.config.max_section_length
            
            # If adding this paragraph exceeds limit, save current chunk
            if current_length + paragraph_length > max_chunk_size and current_chunk:
                chunk_content = '\n\n'.join(current_chunk)
                chunks.append({
                    'content': chunk_content,
                    'section': f'adaptive_chunk_{chunk_id}',
                    'section_type': 'content'
                })
                
                current_chunk = [paragraph]
                current_length = paragraph_length
                chunk_id += 1
            else:
                current_chunk.append(paragraph)
                current_length += paragraph_length
        
        # Add final chunk
        if current_chunk:
            chunk_content = '\n\n'.join(current_chunk)
            chunks.append({
                'content': chunk_content,
                'section': f'adaptive_chunk_{chunk_id}',
                'section_type': 'content'
            })
        
        return chunks

    def _generate_adaptive_metadata(self, chunk: Dict, full_content: Dict) -> Dict:
        """Generate adaptive metadata for chunks"""
        content = chunk.get('content', '')
        
        metadata = {
            'char_count': len(content),
            'word_count': len(content.split()),
            'line_count': len(content.split('\n')),
            'section_type': chunk.get('section_type', 'content'),
            'confidence': chunk.get('confidence', 0.5),
            'contains_tables': bool(re.search(r'\|.*\|', content)),
            'contains_lists': bool(re.search(r'^[\s]*[-â€¢*â—¦â–ªâ–«]\s+', content, re.MULTILINE)),
            'contains_code': bool(re.search(r'\b[A-Z][A-Z0-9_]+\.[A-Z]', content)),
            'technical_density': self._calculate_technical_density(content),
            'business_relevance': self._calculate_business_relevance(content)
        }
        
        return metadata

    def _calculate_technical_density(self, content: str) -> float:
        """Calculate technical content density"""
        technical_indicators = ['database', 'system', 'server', 'configuration', 'parameter', 'variable', 'function', 'method', 'procedure', 'algorithm']
        
        words = content.lower().split()
        technical_count = sum(1 for word in words if any(indicator in word for indicator in technical_indicators))
        
        return min(1.0, technical_count / len(words)) if words else 0.0

    def _calculate_business_relevance(self, content: str) -> float:
        """Calculate business content relevance"""
        business_indicators = ['requirement', 'process', 'workflow', 'business', 'operation', 'procedure', 'policy', 'rule', 'objective', 'goal']
        
        words = content.lower().split()
        business_count = sum(1 for word in words if any(indicator in word for indicator in business_indicators))
        
        return min(1.0, business_count / len(words)) if words else 0.0

    def _extract_business_context(self, content: str) -> Dict:
        """Extract business context from chunk content"""
        context = {
            'key_entities': self._extract_key_entities(content),
            'action_verbs': self._extract_action_verbs(content),
            'domain_terms': self._extract_domain_terms(content),
            'relationships': self._extract_relationships(content)
        }
        
        return context

    def _extract_key_entities(self, content: str) -> List[str]:
        """Extract key entities (nouns, proper nouns)"""
        # Simple entity extraction - could be enhanced with NLP
        entities = []
        
        # Proper nouns (capitalized words)
        proper_nouns = re.findall(r'\b[A-Z][a-z]+\b', content)
        entities.extend(list(set(proper_nouns)))
        
        # Technical identifiers
        tech_ids = re.findall(r'\b[A-Z][A-Z0-9_]+\b', content)
        entities.extend(list(set(tech_ids)))
        
        return entities[:10]  # Top 10 entities

    def _extract_action_verbs(self, content: str) -> List[str]:
        """Extract action verbs that indicate processes"""
        # Common action verbs in business/technical contexts
        action_patterns = [
            r'\b(create|generate|build|develop|implement)\b',
            r'\b(process|execute|perform|run|operate)\b',
            r'\b(transform|convert|translate|map|route)\b',
            r'\b(validate|verify|check|test|monitor)\b',
            r'\b(send|receive|publish|subscribe|transmit)\b'
        ]
        
        verbs = []
        for pattern in action_patterns:
            matches = re.findall(pattern, content.lower())
            verbs.extend(matches)
        
        return list(set(verbs))

    def _extract_domain_terms(self, content: str) -> List[str]:
        """Extract domain-specific terminology"""
        # Look for terms that appear multiple times (likely domain-specific)
        words = re.findall(r'\b[a-z]{4,}\b', content.lower())
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Filter for frequent, non-common words
        common_words = {'that', 'this', 'with', 'from', 'they', 'were', 'been', 'have', 'will', 'would', 'could', 'should', 'when', 'where', 'what', 'which'}
        domain_terms = [word for word, freq in word_freq.items() if freq >= 2 and word not in common_words]
        
        return domain_terms[:5]  # Top 5 domain terms

    def _extract_relationships(self, content: str) -> List[str]:
        """Extract relationships between entities"""
        # Simple relationship extraction
        relationship_patterns = [
            r'(\w+)\s+(?:connects|links|integrates)\s+(?:to|with)\s+(\w+)',
            r'(\w+)\s+(?:sends|receives|processes)\s+(\w+)',
            r'(\w+)\s+(?:contains|includes|has)\s+(\w+)'
        ]
        
        relationships = []
        for pattern in relationship_patterns:
            matches = re.findall(pattern, content.lower())
            for match in matches:
                relationships.append(f"{match[0]} -> {match[1]}")
        
        return relationships[:5]  # Top 5 relationships

    # ================================================================================================
    # UTILITY METHODS
    # ================================================================================================

    def _is_numeric(self, value: str) -> bool:
        """Check if value is numeric"""
        try:
            float(value.replace(',', ''))
            return True
        except (ValueError, AttributeError):
            return False

    def _is_date(self, value: str) -> bool:
        """Check if value looks like a date"""
        date_patterns = [
            r'\d{1,2}[/-]\d{1,2}[/-]\d{2,4}',
            r'\d{4}[/-]\d{1,2}[/-]\d{1,2}',
            r'\b\d{1,2}\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)',
        ]
        
        value_str = str(value).lower()
        return any(re.search(pattern, value_str) for pattern in date_patterns)

    # ================================================================================================
    # PLACEHOLDER METHODS FOR FUTURE IMPLEMENTATION
    # ================================================================================================

    def _extract_diagrams_with_vision(self, pdf_path: str) -> List[Dict]:
        """Extract and analyze diagrams - placeholder for diagram processing"""
        # This would implement the sophisticated diagram processing from your original code
        return []

    def _discover_business_patterns(self, chunks: List[Dict]) -> List[BusinessBlock]:
        """Discover business patterns automatically - to be implemented"""
        # This would implement adaptive business pattern discovery
        return []

    def _enhance_chunks_with_business_data(self, chunks: List[Dict], business_blocks: List[BusinessBlock]) -> List[Dict]:
        """Enhance chunks with business data - to be implemented"""
        # This would integrate discovered business patterns with chunks
        return chunks

    def _prepare_adaptive_vector_chunks(self, chunks: List[Dict]) -> List[Dict]:
        """Prepare chunks for vector store - to be implemented"""
        # This would prepare chunks with adaptive metadata for vector storage
        return chunks

    def _update_processing_statistics(self, chunks: List[Dict], business_blocks: List[BusinessBlock]):
        """Update processing statistics"""
        self.processing_stats.update({
            'total_sections': len(chunks),
            'business_blocks_found': len(business_blocks),
            'patterns_discovered': len(self.pattern_cache)
        })

# ================================================================================================
# EXCEPTION CLASSES
# ================================================================================================

class PDFProcessingError(Exception):
    """Custom exception for PDF processing errors"""
    pass

# ================================================================================================
# END OF ADAPTIVE PDF PROCESSOR
# ================================================================================================