#!/usr/bin/env python3
"""
Schema Generator Module v1.0 - ACE Module Creator
Purpose: Extract schema requirements from Confluence PDF + component mapping JSON + LLM 
         Generate ACE-compatible XSD files with ZERO hardcoded fallbacks
Input: Confluence PDF + component mapping JSON + LLM analysis
Output: Creates schemas/ folder with all schema files needed for the project
"""

import os
import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
from groq import Groq
from dotenv import load_dotenv
import streamlit as st

load_dotenv()

class SchemaGenerator:
    """
    ACE Schema Generator with complete LLM integration
    NO HARDCODED FALLBACKS - All schemas generated via LLM analysis
    """
    
    def __init__(self, groq_api_key: str = None):
        """Initialize with Groq LLM client"""
        self.groq_api_key = groq_api_key or os.getenv('GROQ_API_KEY')
        if not self.groq_api_key:
            raise ValueError("GROQ_API_KEY must be provided or set in environment")
        
        self.llm = Groq(api_key=self.groq_api_key)
        self.groq_model = os.getenv('GROQ_MODEL', 'llama-3.1-8b-instant')
        
        # Processing results tracking
        self.processed_pdf_content = None
        self.processed_json_mappings = None
        self.generated_schemas = []
        self.llm_analysis_calls = 0
        self.llm_generation_calls = 0
    
    def generate_schemas(self, 
                    vector_content: str,  # New: Vector DB content instead of PDF path
                    component_mapping_json_path: str,
                    output_dir: str) -> Dict[str, Any]:
        """
        Main entry point - Generate all XSD schemas using LLM analysis
        
        Args:
            confluence_pdf_path: Path to Confluence PDF with business requirements
            component_mapping_json_path: Path to component mapping JSON
            output_dir: Output directory for generated schemas
            
        Returns:
            Dict with generation results and metadata
        """
        print("ğŸ¯ Starting Schema Generation - Full LLM Processing Mode")
        print("ğŸ“‹ NO HARDCODED FALLBACKS - Pure AI-driven schema extraction")
        
        # Step 1: Process ALL inputs completely
        print("\nğŸ“„ Step 1: Processing ALL inputs for LLM analysis...")
        vector_processed_content = self._process_vector_content(vector_content)
        json_mappings = self._extract_complete_json_mappings(component_mapping_json_path)
        
        # Step 2: LLM Analysis of requirements
        print("\nğŸ§  Step 2: LLM analyzing schema requirements...")
        schema_requirements = self._llm_analyze_schema_requirements(vector_processed_content, json_mappings)
        
        # Step 3: LLM Generation of XSD schemas
        print("\nâš¡ Step 3: LLM generating ACE-compatible XSD schemas...")
        generated_schemas = self._llm_generate_xsd_schemas(schema_requirements, output_dir)
        
        # Step 4: Create output structure
        schemas_dir = os.path.join(output_dir, 'schemas')
        os.makedirs(schemas_dir, exist_ok=True)
        
        # Step 5: Write generated schemas to files
        print("\nğŸ’¾ Step 5: Writing generated schemas to files...")
        schema_files = self._write_schema_files(generated_schemas, schemas_dir)
        
        return {
            'status': 'success',
            'schemas_generated': len(schema_files),
            'schema_files': schema_files,
            'output_directory': schemas_dir,
            'llm_analysis_calls': self.llm_analysis_calls,
            'llm_generation_calls': self.llm_generation_calls,
            'processing_metadata': {
            'vector_content_length': len(vector_content),
            'schema_focus_keywords': vector_processed_content.get('focus_keywords', []),
            'json_components_processed': len(json_mappings.get('components', [])),
            'timestamp': datetime.now().isoformat()
        }
        }
    
    def _process_vector_content(self, vector_content: str) -> Dict[str, Any]:
        """
        Process Vector DB content for schema analysis
        Simple wrapper around Vector DB focused content
        """
        return {
            'schema_content': vector_content,
            'content_length': len(vector_content),
            'focus_keywords': ['schema', 'xsd', 'data structure', 'validation'],
            'data_structures': self._extract_data_structure_hints(vector_content),
            'source': 'vector_db'
        }

    def _extract_data_structure_hints(self, content: str) -> List[str]:
        """Extract data structure patterns from Vector content"""
        patterns = ['CDM', 'Document', 'Message', 'Header', 'Target', 'Source']
        found = [pattern for pattern in patterns if pattern.lower() in content.lower()]
        return found
    
    def _get_vector_content_for_schema_generator(self):
        """Get Vector DB content focused on schema requirements"""
        if not hasattr(self, 'vector_pipeline') or not self.vector_pipeline:
            # Get from session state (already established)
            import streamlit as st
            if not st.session_state.get('vector_pipeline'):
                raise Exception("Vector DB pipeline not available")
            return st.session_state.vector_pipeline.search_engine.get_agent_content("schema_generator")
        return self.vector_pipeline.search_engine.get_agent_content("schema_generator")
    
    def _extract_complete_json_mappings(self, json_path: str) -> Dict[str, Any]:
        """
        Extract COMPLETE JSON mapping content for LLM processing
        NO filtering - all mapping data goes to LLM
        """
        print(f"  ğŸ—‚ï¸ Extracting complete JSON mappings from: {json_path}")
        
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"Component mapping JSON not found: {json_path}")
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            # Validate JSON structure
            if not isinstance(json_data, dict):
                raise ValueError("JSON must be a dictionary object")
            
            print(f"  âœ… JSON processed: {len(str(json_data))} characters")
            self.processed_json_mappings = json_data
            return json_data
            
        except json.JSONDecodeError as e:
            raise Exception(f"Invalid JSON format: {str(e)}")
        except Exception as e:
            raise Exception(f"Failed to process JSON: {str(e)}")
        
        
    
    def _llm_analyze_schema_requirements(self, vector_processed_content: Dict, json_mappings: Dict) -> Dict[str, Any]:
        """
        LLM analyzes ALL content to extract schema requirements
        NO hardcoded analysis - pure LLM intelligence
        """
        print("  ğŸ§  LLM analyzing schema requirements from ALL inputs...")
        
        system_prompt = """You are an expert IBM ACE (App Connect Enterprise) schema architect with deep expertise in XSD schema design and BizTalk-to-ACE migrations.

        Your task is to analyze the complete business requirements and component mappings to extract ALL schema requirements needed for an ACE project.

        ANALYSIS FOCUS:
        - Identify all data structures mentioned in the business requirements
        - Extract entity relationships and data flows  
        - Determine message formats and transformation requirements
        - Identify database schemas and external system interfaces
        - Map BizTalk components to required ACE schemas
        - Consider validation rules and constraints
        - Analyze error handling and exception schemas

        CRITICAL: You MUST return ONLY valid JSON in the EXACT format specified below. No explanations, no markdown, no additional text.

        REQUIRED JSON STRUCTURE:
        {
        "schema_requirements": [
            {
            "name": "SchemaName",
            "description": "Purpose and usage description",
            "category": "message|database|transformation|error|interface",
            "priority": "high|medium|low",
            "fields": [
                {
                "name": "fieldName",
                "type": "string|integer|decimal|boolean|date|complex",
                "required": true/false,
                "description": "Field purpose"
                }
            ],
            "constraints": ["validation rule 1", "validation rule 2"],
            "ace_considerations": "ACE-specific implementation notes"
            }
        ],
        "data_structures": [
            {
            "entity": "EntityName",
            "relationships": ["related entity 1", "related entity 2"],
            "source": "biztalk|database|external_system",
            "target_schema": "corresponding schema name"
            }
        ],
        "validation_rules": [
            {
            "schema": "schema name",
            "rule": "validation rule description",
            "type": "business|technical|format"
            }
        ],
        "relationships": [
            {
            "source_schema": "Schema1",
            "target_schema": "Schema2", 
            "relationship_type": "transformation|lookup|reference|aggregation",
            "description": "How schemas relate"
            }
        ],
        "ace_considerations": [
            {
            "schema": "schema name",
            "consideration": "ACE-specific implementation note",
            "impact": "performance|security|integration|maintenance"
            }
        ]
        }

        EXAMPLE OUTPUT:
        {
        "schema_requirements": [
            {
            "name": "CDMDocument",
            "description": "Common Data Model for customer messages",
            "category": "message",
            "priority": "high",
            "fields": [
                {
                "name": "customerId",
                "type": "string",
                "required": true,
                "description": "Unique customer identifier"
                },
                {
                "name": "documentType",
                "type": "string", 
                "required": true,
                "description": "Type of document being processed"
                }
            ],
            "constraints": ["customerId must be alphanumeric", "documentType from predefined list"],
            "ace_considerations": "Use ACE message sets for validation"
            }
        ],
        "data_structures": [
            {
            "entity": "Customer",
            "relationships": ["Order", "Address"],
            "source": "biztalk",
            "target_schema": "CDMDocument"
            }
        ],
        "validation_rules": [
            {
            "schema": "CDMDocument",
            "rule": "Customer ID must be 8-12 characters",
            "type": "business"
            }
        ],
        "relationships": [
            {
            "source_schema": "CDMDocument",
            "target_schema": "CW1EventFormat",
            "relationship_type": "transformation",
            "description": "CDM transforms to CW1 universal event format"
            }
        ],
        "ace_considerations": [
            {
            "schema": "CDMDocument",
            "consideration": "Implement as shared library for reusability",
            "impact": "maintenance"
            }
        ]
        }

        Be thorough and capture EVERY schema requirement from the provided content. Return ONLY the JSON structure above."""

        user_prompt = f"""Analyze the complete business requirements and component mappings to extract ALL schema requirements:

    ## VECTOR DB BUSINESS REQUIREMENTS:
    **Schema-Focused Content ({len(vector_processed_content.get('schema_content', ''))} characters):**
    {vector_processed_content.get('schema_content', '')}
    **Data Structure Requirements:**
    {json.dumps(vector_processed_content.get('data_structures', []), indent=2)}

    **Focus Keywords:**
    {json.dumps(vector_processed_content.get('focus_keywords', []), indent=2)}

    ## COMPONENT MAPPING JSON:
    {json.dumps(json_mappings, indent=2)}

    ## ANALYSIS REQUIREMENTS:
    Extract ALL schema requirements and return comprehensive JSON with:
    1. schema_requirements: List of all required schemas
    2. data_structures: Detailed structure for each schema
    3. validation_rules: Business rules and constraints
    4. relationships: How schemas relate to each other
    5. ace_considerations: ACE-specific implementation notes

    Analyze every piece of information provided and ensure no schema requirement is missed."""

        try:
            response = self.llm.chat.completions.create(
                model=self.groq_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=3000
            )
            
            # Token tracking (if available)
            if 'token_tracker' in st.session_state and hasattr(response, 'usage') and response.usage:
                st.session_state.token_tracker.manual_track(
                    agent="schema_generator",
                    operation="schema_analysis", 
                    model=self.groq_model,
                    input_tokens=response.usage.prompt_tokens,
                    output_tokens=response.usage.completion_tokens,
                    flow_name=getattr(self, 'current_schema_name', 'schema_analysis')
                )
            
            raw_analysis = response.choices[0].message.content.strip()
            self.llm_analysis_calls += 1
            
            # Parse LLM response as JSON
            try:
                schema_requirements = json.loads(raw_analysis)
                print(f"  âœ… LLM analysis complete: {len(schema_requirements.get('schema_requirements', []))} schemas identified")
                return schema_requirements
            except json.JSONDecodeError:
                # If not valid JSON, try to extract JSON from response
                json_match = re.search(r'\{.*\}', raw_analysis, re.DOTALL)
                if json_match:
                    schema_requirements = json.loads(json_match.group())
                    print(f"  âœ… LLM analysis complete: {len(schema_requirements.get('schema_requirements', []))} schemas identified")
                    return schema_requirements
                else:
                    raise Exception("LLM did not return valid JSON analysis")
                    
        except Exception as e:
            raise Exception(f"LLM schema analysis failed: {str(e)}")
    
    def _llm_generate_xsd_schemas(self, schema_requirements: Dict, output_dir: str) -> List[Dict]:
        """
        LLM generates actual XSD schemas based on requirements analysis
        FIXED: Type validation to prevent 'str' object has no attribute 'get' error
        NO hardcoded templates - pure LLM XSD generation
        """
        print("  âš¡ LLM generating ACE-compatible XSD schemas...")
        
        schemas = schema_requirements.get('schema_requirements', [])
        if not schemas:
            raise Exception("No schema requirements found from LLM analysis")
        
        generated_schemas = []
        
        for schema_req in schemas:
            # FIX: Type validation to prevent 'str' object has no attribute 'get' error
            if not isinstance(schema_req, dict):
                print(f"    âš ï¸ Warning: Schema requirement is not a dict (type: {type(schema_req)}), skipping")
                continue
            
            schema_name = schema_req.get('name', 'UnknownSchema')
            print(f"    ğŸ”¨ Generating {schema_name}.xsd...")
            
            system_prompt = """You are an expert XSD schema designer specializing in IBM ACE (App Connect Enterprise) schemas.

    Generate production-ready XSD schemas that are:
    - Fully compliant with W3C XSD standards
    - Optimized for IBM ACE message processing
    - Include proper namespaces and declarations
    - Include comprehensive validation rules
    - Follow enterprise schema design patterns
    - Include detailed annotations and documentation

    Return ONLY the complete XSD content, no explanations or markdown."""

            user_prompt = f"""Generate a complete XSD schema based on these requirements:

    ## SCHEMA SPECIFICATION:
    **Name:** {schema_name}
    **Purpose:** {schema_req.get('purpose', 'Data structure definition')}
    **Target Namespace:** urn:schemas:ace:{schema_name.lower()}

    ## DATA STRUCTURE REQUIREMENTS:
    {json.dumps(schema_req.get('data_structure', {}), indent=2)}

    ## VALIDATION RULES:
    {json.dumps(schema_req.get('validation_rules', []), indent=2)}

    ## BUSINESS CONTEXT:
    {schema_req.get('business_context', 'Enterprise data exchange')}

    ## ACE CONSIDERATIONS:
    {json.dumps(schema_req.get('ace_considerations', {}), indent=2)}

    Generate the complete XSD schema content that satisfies all these requirements."""

            try:
                response = self.llm.chat.completions.create(
                    model=self.groq_model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.2,
                    max_tokens=2500
                )
                
                if 'token_tracker' in st.session_state and hasattr(response, 'usage') and response.usage:
                    st.session_state.token_tracker.manual_track(
                        agent="schema_generator",
                        operation="schema_generation",
                        model=self.groq_model,
                        input_tokens=response.usage.prompt_tokens,
                        output_tokens=response.usage.completion_tokens,
                        flow_name=getattr(self, 'current_schema_name', 'schema_gen')
                    )
                
                xsd_content = response.choices[0].message.content.strip()
                self.llm_generation_calls += 1
                
                # Clean up any markdown formatting if present
                xsd_content = re.sub(r'^```xml\s*\n?', '', xsd_content)
                xsd_content = re.sub(r'\n?```\s*$', '', xsd_content)
                
                generated_schemas.append({
                    'name': schema_name,
                    'filename': f"{schema_name}.xsd",
                    'content': xsd_content,
                    'requirements': schema_req
                })
                
                print(f"    âœ… Generated {schema_name}.xsd ({len(xsd_content)} characters)")
                
            except Exception as e:
                raise Exception(f"Failed to generate XSD for {schema_name}: {str(e)}")
        
        print(f"  ğŸ‰ All schemas generated: {len(generated_schemas)} XSD files")
        return generated_schemas
    
    def _write_schema_files(self, generated_schemas: List[Dict], schemas_dir: str) -> List[str]:
        """Write generated XSD schemas to files"""
        print("  ğŸ’¾ Writing XSD schemas to files...")
        
        schema_files = []
        
        for schema in generated_schemas:
            filename = schema['filename']
            content = schema['content']
            
            file_path = os.path.join(schemas_dir, filename)
            
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                schema_files.append(file_path)
                print(f"    ğŸ“„ Created: {filename}")
                
            except Exception as e:
                print(f"    âŒ Failed to write {filename}: {str(e)}")
                raise
        
        print(f"  âœ… All schema files written to: {schemas_dir}")
        return schema_files

def main():
    """Test harness for schema generator - Vector DB Mode"""
    generator = SchemaGenerator()
    
    # Get Vector DB content using existing pipeline (NO PDF processing)
    import streamlit as st
    vector_content = st.session_state.vector_pipeline.search_engine.get_agent_content("schema_generator")
    
    # Generate schemas using Vector DB content
    result = generator.generate_schemas(
        vector_content=vector_content,                      # âœ… Vector DB content
        component_mapping_json_path="component_mapping.json",  # âœ… Direct path for testing
        output_dir="test_output"                           # âœ… Direct path for testing
    )
    
    print(f"\nğŸ¯ Schema Generation Results:")
    print(f"âœ… Status: {result['status']}")
    print(f"ğŸ“Š Schemas Generated: {result['schemas_generated']}")
    print(f"ğŸ§  LLM Analysis Calls: {result['llm_analysis_calls']}")
    print(f"âš¡ LLM Generation Calls: {result['llm_generation_calls']}")


if __name__ == "__main__":
    main()