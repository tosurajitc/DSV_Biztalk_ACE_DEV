#!/usr/bin/env python3
"""
Project Generator Module v1.0 - ACE Module Creator
Purpose: Generate final .project file with all dependencies
Input: project.xml template + component mapping JSON + LLM â†’ Generate final .project file with all dependencies
Output: Creates .project file for IBM ACE Toolkit compatibility with proper build configurations
Prompt Focus: "Use project template, analyze all generated components, create final .project file with proper build configurations"
LLM Task: Project file generation with dependency management
"""

import os
import json
import re
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
from groq import Groq
from dotenv import load_dotenv
import streamlit as st
import sys
load_dotenv()

class ProjectGenerator:
    """
    ACE Project Generator with complete LLM integration
    NO HARDCODED FALLBACKS - All project file generation via LLM analysis
    """
    
    def __init__(self, groq_api_key: str = None):
        """Initialize with Groq LLM client"""
        self.groq_api_key = groq_api_key or os.getenv('GROQ_API_KEY')
        if not self.groq_api_key:
            raise ValueError("GROQ_API_KEY must be provided or set in environment")
        
        self.llm = Groq(api_key=self.groq_api_key)
        self.groq_model = os.getenv('GROQ_MODEL', 'llama-3.1-8b-instant')
        
        # Processing results tracking
        self.processed_template_content = None
        self.processed_json_mappings = None
        self.analyzed_components = None
        self.generated_project_file = None
        self.llm_analysis_calls = 0
        self.llm_generation_calls = 0


    
    def generate_project_file(self, 
                            vector_content: str,
                            template_path: str,
                            component_mapping_json_path: str,
                            output_dir: str,
                            biztalk_folder: str = None,
                            generated_components_dir: str = None) -> Dict[str, Any]:
        """Generate final .project file - FORCE BizTalk parsing for debugging"""
        
        print("ðŸŽ¯ Starting Project File Generation - DEBUG MODE")
        
        # Extract project name
        project_name = os.path.basename(os.path.abspath(output_dir))
        if not project_name or project_name == '.':
            raise Exception("Invalid output directory - cannot extract project name")
        
        # âœ… FORCE BIZTALK PARSING - NO CONDITIONS
        print("ðŸ”§ DEBUG: FORCING BizTalk parsing unconditionally...")
        print(f"  ðŸ“‚ biztalk_folder parameter: {repr(biztalk_folder)}")
        print(f"  ðŸ“Š vector_content length: {len(vector_content)}")
        print(f"  ðŸ“‹ vector_content preview: {repr(vector_content[:100])}")
        
        if biztalk_folder:
            print("ðŸš€ FORCING BizTalk project analysis...")
            try:
                biztalk_content = self._parse_biztalk_project_structure(biztalk_folder, project_name)
                print(f"  âœ… BizTalk content generated: {len(biztalk_content)} characters")
                
                # âœ… USE ONLY BIZTALK CONTENT (ignore Vector DB completely)
                content_for_llm = biztalk_content
                content_source = "forced_biztalk_only"
                
            except Exception as e:
                print(f"  âŒ BizTalk parsing failed: {e}")
                print("  ðŸ”„ Falling back to Vector DB content...")
                content_for_llm = vector_content
                content_source = "biztalk_failed_vector_fallback"
        else:
            print("âŒ No BizTalk folder provided - using Vector DB content")
            content_for_llm = vector_content
            content_source = "no_biztalk_folder"
        
        # âœ… ENHANCED DEBUG OUTPUT
        print(f"ðŸ“Š Final content for LLM:")
        print(f"  ðŸ“ Source: {content_source}")
        print(f"  ðŸ“ Length: {len(content_for_llm)} characters")
        print(f"  ðŸ“‹ Preview: {repr(content_for_llm[:150])}...")
        print(f"  ðŸŽ¯ Project name: {project_name}")
        
        # Process template and mappings
        print("ðŸ“„ Processing template and mappings...")
        template_content = self._extract_complete_template_content(template_path)
        json_mappings = self._extract_complete_json_mappings(component_mapping_json_path)
        
        # Analyze components
        print("ðŸ” Analyzing generated components...")
        if generated_components_dir and os.path.exists(generated_components_dir):
            component_analysis = self._analyze_generated_components(generated_components_dir)
        else:
            component_analysis = {'generated_components': [], 'component_summary': 'No components'}
        
        print(f"  ðŸ“¦ Component analysis: {len(component_analysis.get('generated_components', []))} files")
        
        # LLM Analysis with forced content
        print("ðŸ§  Starting LLM analysis with forced content...")
        project_analysis = self._llm_analyze_project_requirements_vector(
            content_for_llm,  # âœ… Now forced to use BizTalk content (2000+ chars)
            template_content, 
            json_mappings, 
            component_analysis,
            project_name
        )
        
        # Generate and write project file
        print("âš¡ Generating final .project file...")
        generated_project = self._llm_generate_project_file(project_analysis, template_content, project_name)
        
        print("ðŸ’¾ Writing .project file...")
        project_file_path = self._write_project_file(generated_project, output_dir)
        
        print("âœ… Project generation completed!")
        print(f"ðŸ“Š Summary:")
        print(f"  ðŸ“ Project file: {project_file_path}")
        print(f"  ðŸ“ Content source: {content_source}")
        print(f"  ðŸ“ Content length: {len(content_for_llm)} characters")
        print(f"  ðŸ§  LLM calls: {self.llm_analysis_calls} analysis + {self.llm_generation_calls} generation")
        
        return {
            'status': 'success',
            'project_file_path': project_file_path,
            'project_name': project_name,
            'project_file_generated': True,
            'content_source': content_source,  # âœ… Track what content was actually used
            'content_length': len(content_for_llm),  # âœ… Track content size for debugging
            'biztalk_folder_provided': biztalk_folder is not None,  # âœ… Debug info
            'biztalk_parsing_attempted': biztalk_folder is not None,  # âœ… Debug info
            'llm_analysis_calls': self.llm_analysis_calls,
            'llm_generation_calls': self.llm_generation_calls
        }
    


    def _parse_biztalk_project_structure(self, biztalk_folder: str, project_name: str) -> str:
        """
        Parse BizTalk project structure to extract ACE requirements
        âœ… This is the ONLY missing method needed!
        """
        print(f"ðŸ” Parsing BizTalk folder: {biztalk_folder}")
        
        if not os.path.exists(biztalk_folder):
            print(f"âŒ BizTalk folder not found: {biztalk_folder}")
            return f"BizTalk folder not found: {biztalk_folder}"
        
        try:
            # Find .btproj files
            btproj_files = []
            for root, dirs, files in os.walk(biztalk_folder):
                for file in files:
                    if file.lower().endswith('.btproj'):
                        btproj_path = os.path.join(root, file)
                        btproj_files.append(btproj_path)
                        print(f"ðŸ“„ Found .btproj file: {file}")
            
            if not btproj_files:
                print("âš ï¸ No .btproj files found")
                return f"No .btproj files found in {biztalk_folder}"
            
            # Read the first .btproj file
            with open(btproj_files[0], 'r', encoding='utf-8') as f:
                btproj_content = f.read()
            
            print(f"ðŸ“„ Read .btproj file: {len(btproj_content)} characters")
            
            # Create rich content for LLM with actual BizTalk project data
            rich_content = f"""
    # BizTalk to IBM ACE Project Migration Analysis

    ## PROJECT DETAILS:
    - **Target Project Name**: {project_name}
    - **Source**: BizTalk Server Project Migration
    - **BizTalk Project File**: {os.path.basename(btproj_files[0])}

    ## BIZTALK PROJECT CONTENT ANALYSIS:
    {btproj_content[:2000]}

    ## IBM ACE PROJECT REQUIREMENTS:

    ### Mandatory Dependencies:
    - EPIS_CommonUtils_Lib
    - EPIS_Consumer_Lib_v2
    - EPIS_BlobStorage_Lib
    - EPIS_MessageEnrichment_StaticLib
    - EPIS_CommonFlows_Lib
    - EPIS_CargoWiseOne_eAdapter_Lib
    - EPIS_CargoWiseOne_Schemas_Lib

    ### Project Configuration:
    - **Project Name**: {project_name}
    - **Project Type**: IBM ACE Application Library
    - **Build Target**: Enterprise Integration
    - **Deployment**: BAR file generation

    ### Migration Context:
    This project migrates BizTalk Server integration components to IBM App Connect Enterprise (ACE) with enterprise message processing, transformation logic, and CargoWise integration patterns.

    ### Technical Architecture:
    - Message Flows: Document processing workflows
    - ESQL Modules: Business logic implementation
    - Database Integration: Lookup and enrichment operations
    - Error Handling: Comprehensive error processing
    - Monitoring: Transaction tracking and logging
    """
            
            print(f"âœ… Generated rich BizTalk content: {len(rich_content)} characters")
            return rich_content.strip()
            
        except Exception as e:
            print(f"âŒ BizTalk parsing failed: {e}")
            # Return rich fallback content instead of simple error
            return f"""
    # BizTalk Project Analysis - Error Recovery

    ## PROJECT DETAILS:
    - **Target Project Name**: {project_name}
    - **Error**: {str(e)}

    ## IBM ACE PROJECT REQUIREMENTS (Fallback):

    ### Mandatory Dependencies:
    - EPIS_CommonUtils_Lib
    - EPIS_Consumer_Lib_v2
    - EPIS_BlobStorage_Lib
    - EPIS_MessageEnrichment_StaticLib
    - EPIS_CommonFlows_Lib
    - EPIS_CargoWiseOne_eAdapter_Lib
    - EPIS_CargoWiseOne_Schemas_Lib

    ### Project Configuration:
    - **Project Name**: {project_name}
    - **Project Type**: IBM ACE Application Library
    - **Migration Source**: BizTalk Server (error during analysis)

    This provides minimum viable project configuration for IBM ACE with all mandatory enterprise integration libraries.
    """


    def _llm_analyze_project_requirements_vector(self, vector_content: str, template_content: Dict, 
                                            json_mappings: Dict, component_analysis: Dict, 
                                            project_name: str) -> Dict[str, Any]:
        """
        LLM analysis of Vector DB content for project architecture requirements
        ENFORCES: Standard libraries, required natures, project name validation
        """
        
        try:
            system_prompt = """You are an IBM ACE project architect. Analyze Vector DB business requirements to determine .project file requirements.

                MANDATORY REQUIREMENTS (MUST include ALL):
                1. Project name MUST be: {project_name}
                2. Standard libraries MUST include ALL 7: EPIS_CommonUtils_Lib, EPIS_Consumer_Lib_v2, EPIS_BlobStorage_Lib, EPIS_MessageEnrichment_StaticLib, EPIS_CommonFlows_Lib, EPIS_CargoWiseOne_eAdapter_Lib, EPIS_CargoWiseOne_Schemas_Lib
                3. Natures MUST include: com.ibm.etools.mft.applib.applibrarynature, com.ibm.etools.mft.project.partnature

                Return ONLY valid JSON:
                {{
                "project_name": "{project_name}",
                "project_dependencies": ["all 7 standard libraries plus any additional from analysis"],
                "natures": ["required natures plus any additional from analysis"],
                "builders": ["extracted from template"],
                "additional_dependencies": ["any extra dependencies from Vector DB analysis"]
                }}""".format(project_name=project_name)
            
            print("ðŸ” DEBUG: System prompt created successfully")

            user_prompt = f"""Analyze ALL inputs for IBM ACE project requirements for project: {project_name}
        
    ## VECTOR DB BUSINESS REQUIREMENTS:
    {vector_content[:3000]}

    ## PROJECT TEMPLATE:
    {json.dumps(template_content, indent=2)[:2000]}

    ## COMPONENT MAPPINGS:
    {json.dumps(json_mappings, indent=2)[:2000]}

    ## GENERATED COMPONENTS:
    {json.dumps(component_analysis, indent=2)[:1000]}

    Extract project requirements ensuring ALL mandatory libraries and natures are included. Return ONLY JSON."""
        
            print(f"ðŸ” DEBUG: System prompt length: {len(system_prompt)}")
            print(f"ðŸ” DEBUG: User prompt length: {len(user_prompt)}")

        except Exception as e:
            print(f"ðŸ” DEBUG: Exception caught in method: {e}")
            raise e

        try:
            response = self.llm.chat.completions.create(
                model=self.groq_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=3000
            )
            print(f"DEBUG: Full LLM response: {repr(response.choices[0].message.content)}")
            
            if 'token_tracker' in st.session_state and hasattr(response, 'usage') and response.usage:
                st.session_state.token_tracker.manual_track(
                    agent="project_generator",
                    operation="vector_project_analysis",
                    model=self.groq_model,
                    input_tokens=response.usage.prompt_tokens,
                    output_tokens=response.usage.completion_tokens,
                    flow_name=project_name
                )
            
            raw_analysis = response.choices[0].message.content.strip()
            self.llm_analysis_calls += 1
            
            try:
                project_analysis = json.loads(raw_analysis)
                print(f"  âœ… Vector DB project analysis complete: {len(project_analysis.get('project_dependencies', []))} dependencies identified")
                return project_analysis
            except json.JSONDecodeError:
                json_content = self._extract_json_from_response(raw_analysis)
                if json_content:
                    project_analysis = json.loads(json_content)
                    print(f"  âœ… Vector DB project analysis complete: {len(project_analysis.get('project_dependencies', []))} dependencies identified")
                    return project_analysis
                else:
                    raise Exception("LLM did not return valid JSON project analysis")

        except Exception as e:
            raise Exception(f"Vector DB LLM project analysis failed: {str(e)}")
    


    def _extract_json_from_response(self, response_text: str) -> str:
        """Extract JSON from markdown-wrapped LLM response"""
        import re
        
        # Remove markdown code blocks
        if response_text.startswith('```json'):
            response_text = response_text[7:]  # Remove '```json\n'
        if response_text.startswith('```'):
            response_text = response_text[3:]   # Remove '```\n'
        if response_text.endswith('```'):
            response_text = response_text[:-3]  # Remove closing '```'
        
        # Find JSON content between braces
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            return json_match.group(0).strip()
        
        return response_text.strip()        


    
    def _extract_complete_template_content(self, template_path: str) -> Dict[str, Any]:
        """
        Extract COMPLETE project template content for LLM processing
        NO filtering - entire template structure goes to LLM for analysis
        """
        print(f"  ðŸ“ Extracting complete project template from: {template_path}")
        
        if not os.path.exists(template_path):
            raise FileNotFoundError(f"Project template not found: {template_path}")
        
        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                template_xml = f.read()
            
            # Parse XML structure for comprehensive analysis
            try:
                root = ET.fromstring(template_xml)
                xml_structure = self._analyze_project_xml_structure(root)
            except ET.ParseError:
                # If not valid XML, treat as text template
                xml_structure = {'type': 'text_template', 'valid_xml': False}
            
            template_data = {
                'raw_content': template_xml,
                'content_length': len(template_xml),
                'xml_structure': xml_structure,
                'build_configurations': self._extract_build_configurations(template_xml),
                'dependency_references': self._extract_dependency_references(template_xml),
                'project_properties': self._extract_project_properties(template_xml),
                'nature_references': self._extract_nature_references(template_xml),
                'builder_configurations': self._extract_builder_configurations(template_xml)
            }
            
            print(f"  âœ… Template processed: {len(template_xml)} characters, {len(template_data['build_configurations'])} build configurations detected")
            self.processed_template_content = template_data
            return template_data
            
        except Exception as e:
            raise Exception(f"Failed to process project template: {str(e)}")
    
    def _extract_complete_json_mappings(self, json_path: str) -> Dict[str, Any]:
        """
        Extract COMPLETE JSON mapping content for project dependency analysis
        NO filtering - all component mapping data goes to LLM for project analysis
        """
        print(f"  ðŸ—‚ï¸ Extracting complete JSON component mappings for project analysis from: {json_path}")
        
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"Component mapping JSON not found: {json_path}")
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            if not isinstance(json_data, dict):
                raise ValueError("JSON must be a dictionary object")
            
            # Enhance JSON data for project dependency analysis
            enhanced_json = {
                'original_data': json_data,
                'project_dependencies': self._extract_project_dependencies(json_data),
                'build_requirements': self._extract_build_requirements(json_data),
                'component_relationships': self._extract_component_relationships(json_data),
                'external_dependencies': self._extract_external_dependencies(json_data),
                'ace_configurations': self._extract_ace_configurations(json_data)
            }
            
            print(f"  âœ… JSON processed: {len(str(json_data))} characters, {len(enhanced_json['project_dependencies'])} project dependencies identified")
            self.processed_json_mappings = enhanced_json
            return enhanced_json
            
        except json.JSONDecodeError as e:
            raise Exception(f"Invalid JSON format: {str(e)}")
        except Exception as e:
            raise Exception(f"Failed to process JSON for project analysis: {str(e)}")
    
    def _analyze_generated_components(self, components_dir: str) -> Dict[str, Any]:
        """
        Analyze all generated ACE components for dependency management
        NO filtering - complete component analysis for LLM processing
        """
        print(f"  ðŸ” Analyzing all generated components from: {components_dir}")
        
        component_analysis = {
            'generated_components': [],
            'component_summary': {},
            'file_dependencies': [],
            'resource_requirements': []
        }
        
        try:
            # Scan all generated files
            for root, dirs, files in os.walk(components_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, components_dir)
                    
                    file_info = {
                        'path': relative_path,
                        'full_path': file_path,
                        'extension': Path(file).suffix,
                        'size': os.path.getsize(file_path),
                        'directory': os.path.dirname(relative_path),
                        'dependencies': self._analyze_file_dependencies(file_path)
                    }
                    
                    component_analysis['generated_components'].append(file_info)
            
            # Create summary by file type
            file_types = {}
            for component in component_analysis['generated_components']:
                ext = component['extension']
                if ext not in file_types:
                    file_types[ext] = []
                file_types[ext].append(component['path'])
            
            component_analysis['component_summary'] = {
                'total_files': len(component_analysis['generated_components']),
                'file_types': file_types,
                'directories': list(set([comp['directory'] for comp in component_analysis['generated_components'] if comp['directory']]))
            }
            
            print(f"  âœ… Component analysis complete: {len(component_analysis['generated_components'])} files analyzed")
            self.analyzed_components = component_analysis
            return component_analysis
            
        except Exception as e:
            print(f"  âš ï¸ Component analysis failed: {str(e)}")
            return {'generated_components': [], 'component_summary': 'Analysis failed', 'error': str(e)}
    

        
    def _llm_generate_project_file(self, project_analysis: Dict, template_content: Dict, project_name: str) -> Dict[str, Any]:
        """
        LLM generate final .project file with enforced standard libraries and natures
        ENFORCES: Exact project name, all 7 standard libraries, required natures
        """
        print(f"  âš¡ LLM generating final .project file for: {project_name}")
        
        system_prompt = f"""Generate IBM ACE .project file using template with MANDATORY enforcement:

    PROJECT NAME: Must be exactly: {project_name}

    MANDATORY LIBRARIES (ALL 7 REQUIRED):
    - EPIS_CommonUtils_Lib
    - EPIS_Consumer_Lib_v2  
    - EPIS_BlobStorage_Lib
    - EPIS_MessageEnrichment_StaticLib
    - EPIS_CommonFlows_Lib
    - EPIS_CargoWiseOne_eAdapter_Lib
    - EPIS_CargoWiseOne_Schemas_Lib

    MANDATORY NATURES (BOTH REQUIRED):
    - com.ibm.etools.mft.applib.applibrarynature
    - com.ibm.etools.mft.project.partnature

    Use template structure exactly. Generate complete .project XML file only."""

        user_prompt = f"""Generate .project file for project: {project_name}

    ## PROJECT ANALYSIS:
    {json.dumps(project_analysis, indent=2)}

    ## TEMPLATE STRUCTURE:
    {json.dumps(template_content, indent=2)[:3000]}

    Requirements:
    1. Use template XML structure exactly
    2. Set project name to: {project_name}
    3. Include ALL 7 mandatory libraries in <projects> section
    4. Include BOTH mandatory natures in <natures> section
    5. Include all builders from template
    6. Add any additional dependencies from analysis

    Generate complete XML .project file:"""

        try:
            response = self.llm.chat.completions.create(
                model=self.groq_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=4000
            )
            
            if 'token_tracker' in st.session_state and hasattr(response, 'usage') and response.usage:
                st.session_state.token_tracker.manual_track(
                    agent="project_generator",
                    operation="project_generation",
                    model=self.groq_model,
                    input_tokens=response.usage.prompt_tokens,
                    output_tokens=response.usage.completion_tokens,
                    flow_name=project_name
                )
            
            project_content = response.choices[0].message.content.strip()
            self.llm_generation_calls += 1
            
            # Clean LLM output - remove code markers if present
            project_content = re.sub(r'```[\w]*\n?', '', project_content)
            project_content = re.sub(r'\n?```\s*$', '', project_content)
            
            # Validate essential elements are present
            if f'<name>{project_name}</name>' not in project_content:
                raise Exception(f"Generated .project file missing required project name: {project_name}")
            
            if '<projects>' not in project_content or '</projects>' not in project_content:
                raise Exception("Generated .project file missing required projects section")
            
            if '<natures>' not in project_content or '</natures>' not in project_content:
                raise Exception("Generated .project file missing required natures section")
            
            generated_project = {
                'content': project_content,
                'content_length': len(project_content),
                'project_analysis': project_analysis,
                'project_name': project_name
            }
            
            print(f"  âœ… Generated .project file ({len(project_content)} characters)")
            self.generated_project_file = generated_project
            return generated_project
            
        except Exception as e:
            raise Exception(f"Failed to generate .project file: {str(e)}")
        


    
    def _write_project_file(self, generated_project: Dict, output_dir: str) -> str:
        """Write generated project file to .project file in project root"""
        print("  ðŸ’¾ Writing .project file for IBM ACE Toolkit compatibility...")
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Write to .project file (note the dot prefix)
        project_file_path = os.path.join(output_dir, '.project')
        
        try:
            with open(project_file_path, 'w', encoding='utf-8') as f:
                f.write(generated_project['content'])
            
            print(f"  âœ… .project file written to: {project_file_path}")
            return project_file_path
            
        except Exception as e:
            print(f"  âŒ Failed to write .project file: {str(e)}")
            raise
    
    # Helper methods for content analysis
    def _analyze_project_xml_structure(self, root: ET.Element) -> Dict[str, Any]:
        """Analyze XML structure of project template"""
        return {
            'root_tag': root.tag,
            'root_attributes': dict(root.attrib),
            'child_elements': [child.tag for child in root],
            'total_elements': len(list(root.iter())),
            'valid_xml': True,
            'project_name': self._extract_project_name(root),
            'natures': self._extract_project_natures(root),
            'builders': self._extract_project_builders(root)
        }
    
    def _extract_build_configurations(self, content: str) -> List[str]:
        """Extract build configuration patterns from template"""
        patterns = [
            r'<buildCommand[^>]*>.*?</buildCommand>',
            r'<builder[^>]*>.*?</builder>',
            r'<buildSpec[^>]*>.*?</buildSpec>'
        ]
        configurations = []
        for pattern in patterns:
            configurations.extend(re.findall(pattern, content, re.IGNORECASE | re.DOTALL))
        return configurations
    
    def _extract_dependency_references(self, content: str) -> List[str]:
        """Extract dependency reference patterns from template"""
        patterns = [
            r'<project[^>]*>([^<]+)</project>',
            r'<dependency[^>]*>([^<]+)</dependency>',
            r'<reference[^>]*>([^<]+)</reference>'
        ]
        dependencies = []
        for pattern in patterns:
            dependencies.extend(re.findall(pattern, content, re.IGNORECASE))
        return list(set(dependencies))
    
    def _extract_project_properties(self, content: str) -> List[str]:
        """Extract project property patterns from template"""
        patterns = [
            r'<property[^>]*>.*?</property>',
            r'<projectProperty[^>]*>.*?</projectProperty>',
            r'<setting[^>]*>.*?</setting>'
        ]
        properties = []
        for pattern in patterns:
            properties.extend(re.findall(pattern, content, re.IGNORECASE | re.DOTALL))
        return properties
    
    def _extract_nature_references(self, content: str) -> List[str]:
        """Extract nature reference patterns from template"""
        patterns = [
            r'<nature>([^<]+)</nature>',
            r'<projectNature>([^<]+)</projectNature>'
        ]
        natures = []
        for pattern in patterns:
            natures.extend(re.findall(pattern, content, re.IGNORECASE))
        return list(set(natures))
    
    def _extract_builder_configurations(self, content: str) -> List[str]:
        """Extract builder configuration patterns from template"""
        patterns = [
            r'<name>([^<]+)</name>',
            r'<arguments[^>]*>.*?</arguments>'
        ]
        builders = []
        for pattern in patterns:
            builders.extend(re.findall(pattern, content, re.IGNORECASE | re.DOTALL))
        return builders
    
    def _extract_project_dependencies(self, json_data: Dict) -> List[Dict]:
        """Extract project dependency information from JSON"""
        dependencies = []
        
        if 'project_dependencies' in json_data:
            dependencies.extend(json_data['project_dependencies'])
        if 'dependencies' in json_data:
            dependencies.extend(json_data['dependencies'])
        if 'components' in json_data:
            for component in json_data['components']:
                if 'project_dependencies' in component:
                    dependencies.extend(component['project_dependencies'])
        
        return dependencies
    
    def _extract_build_requirements(self, json_data: Dict) -> List[Dict]:
        """Extract build requirement information from JSON"""
        requirements = []
        
        if 'build_requirements' in json_data:
            requirements.extend(json_data['build_requirements'])
        if 'build_settings' in json_data:
            requirements.extend(json_data['build_settings'])
        
        return requirements
    
    def _extract_component_relationships(self, json_data: Dict) -> List[Dict]:
        """Extract component relationship information from JSON"""
        relationships = []
        
        if 'component_relationships' in json_data:
            relationships.extend(json_data['component_relationships'])
        if 'components' in json_data:
            for component in json_data['components']:
                if 'relationships' in component:
                    relationships.extend(component['relationships'])
        
        return relationships
    
    def _extract_external_dependencies(self, json_data: Dict) -> List[Dict]:
        """Extract external dependency information from JSON"""
        dependencies = []
        
        if 'external_dependencies' in json_data:
            dependencies.extend(json_data['external_dependencies'])
        if 'external_systems' in json_data:
            dependencies.extend(json_data['external_systems'])
        
        return dependencies
    
    def _extract_ace_configurations(self, json_data: Dict) -> Dict[str, Any]:
        """Extract ACE-specific configuration information from JSON"""
        config = {}
        
        if 'ace_config' in json_data:
            config.update(json_data['ace_config'])
        if 'ace_settings' in json_data:
            config.update(json_data['ace_settings'])
        
        return config
    
    def _analyze_file_dependencies(self, file_path: str) -> List[str]:
        """Analyze file for dependency references"""
        dependencies = []
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Look for common dependency patterns
            dependency_patterns = [
                r'import\s+([^\s;]+)',
                r'require\s*\(\s*["\']([^"\']+)["\']',
                r'<reference[^>]*>([^<]+)</reference>',
                r'SET\s+OutputRoot\.([A-Za-z0-9_\.]+)'
            ]
            
            for pattern in dependency_patterns:
                dependencies.extend(re.findall(pattern, content, re.IGNORECASE))
            
        except:
            pass  # Skip files that can't be read
        
        return list(set(dependencies))
    
    def _extract_project_name(self, root: ET.Element) -> str:
        """Extract project name from XML"""
        name_elem = root.find('name')
        return name_elem.text if name_elem is not None else 'unknown'
    
    def _extract_project_natures(self, root: ET.Element) -> List[str]:
        """Extract project natures from XML"""
        natures = []
        natures_spec = root.find('projectDescription/natures')
        if natures_spec is not None:
            for nature in natures_spec.findall('nature'):
                if nature.text:
                    natures.append(nature.text)
        return natures
    
    def _extract_project_builders(self, root: ET.Element) -> List[str]:
        """Extract project builders from XML"""
        builders = []
        build_spec = root.find('projectDescription/buildSpec')
        if build_spec is not None:
            for build_command in build_spec.findall('buildCommand'):
                name_elem = build_command.find('name')
                if name_elem is not None and name_elem.text:
                    builders.append(name_elem.text)
        return builders
    

    def test_method_version(self):
        """Simple test to verify which version is running"""
        return "ðŸš¨ NEW VERSION DETECTED - UPDATED CODE IS RUNNING! ðŸš¨"


def main():
    """Test harness for project generator - NO HARDCODED PATHS"""
    generator = ProjectGenerator()
    
    # âœ… CORRECTED: Generic test without hardcoded BizTalk path
    result = generator.generate_project_file(
        vector_content="Sample business requirements for testing",  # âœ… Required parameter
        template_path="project_template.xml",                      # Template file
        component_mapping_json_path="component_mapping.json",      # Component mapping
        output_dir="test_output",                                  # Output directory
        biztalk_folder=None,                                       # âœ… No hardcoded path - makes it optional for testing
        generated_components_dir="test_output"                     # Generated components
    )
    
    print(f"\nðŸŽ¯ Project File Generation Results:")
    print(f"âœ… Status: {result['status']}")
    print(f"ðŸ“Š Project File Generated: {result['project_file_generated']}")
    print(f"ðŸ§  LLM Analysis Calls: {result['llm_analysis_calls']}")
    print(f"âš¡ LLM Generation Calls: {result['llm_generation_calls']}")
    
    # âœ… Test with biztalk_folder if you want to test locally (optional)
    if len(sys.argv) > 1:  # Only if BizTalk path is provided as command line argument
        test_biztalk_folder = sys.argv[1]
        print(f"\nðŸ”„ Testing with BizTalk folder: {test_biztalk_folder}")
        
        result_with_biztalk = generator.generate_project_file(
            vector_content="Insufficient content",  # Simulate Vector DB failure
            template_path="project_template.xml",
            component_mapping_json_path="component_mapping.json",
            output_dir="test_output_biztalk",
            biztalk_folder=test_biztalk_folder,      # âœ… From command line
            generated_components_dir="test_output_biztalk"
        )
        
        print(f"âœ… BizTalk Test Status: {result_with_biztalk['status']}")
        print(f"ðŸ“‚ Content Source: {result_with_biztalk.get('content_source', 'N/A')}")


if __name__ == "__main__":
    main()


